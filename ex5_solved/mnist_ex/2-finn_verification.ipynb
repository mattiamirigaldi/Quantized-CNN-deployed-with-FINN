{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import onnx \n",
    "import torch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "build_dir = \"/workspace/finn/notebooks/mnist_ex/verification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import brevitas.onnx as bo\n",
    "from finn.util.visualization import showInNetron\n",
    "from finn.util.test import get_test_model_trained\n",
    "from finn.transformation.streamline import Streamline\n",
    "from finn.transformation.lower_convs_to_matmul import LowerConvsToMatMul\n",
    "from finn.transformation.bipolar_to_xnor import ConvertBipolarMatMulToXnorPopcount\n",
    "import finn.transformation.streamline.absorb as absorb\n",
    "from finn.transformation.streamline.reorder import MakeMaxPoolNHWC, MoveScalarLinearPastInvariants, MoveFlattenPastAffine\n",
    "from finn.transformation.infer_data_layouts import InferDataLayouts\n",
    "from finn.transformation.general import RemoveUnusedTensors\n",
    "\n",
    "import finn.transformation.fpgadataflow.convert_to_hls_layers as to_hls\n",
    "from finn.transformation.fpgadataflow.create_dataflow_partition import (\n",
    "    CreateDataflowPartition,\n",
    ")\n",
    "from finn.transformation.move_reshape import RemoveCNVtoFCFlatten\n",
    "from finn.custom_op.registry import getCustomOp\n",
    "from finn.transformation.infer_data_layouts import InferDataLayouts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import model .onnx into FINN\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "ready_model_filename = \"model_fmnist_notebook.onnx\"\n",
    "model_for_sim = ModelWrapper(ready_model_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor name: inp.1\n",
      "Output tensor name: 45\n",
      "Input tensor shape: [1, 1, 28, 28]\n",
      "Output tensor shape: [1, 10]\n",
      "Input tensor datatype: FLOAT32\n",
      "Output tensor datatype: FLOAT32\n",
      "List of node operator types in the graph: \n",
      "['Conv', 'Mul', 'MultiThreshold', 'Mul', 'MaxPool', 'Conv', 'Mul', 'MultiThreshold', 'Mul', 'MaxPool', 'Conv', 'Mul', 'MultiThreshold', 'Mul', 'Flatten', 'MatMul', 'Mul', 'Div', 'Add', 'Mul']\n"
     ]
    }
   ],
   "source": [
    "#member fuctions used to extract information about the structure and properties of the ONNX model\n",
    "from finn.core.datatype import DataType\n",
    "\n",
    "finnonnx_in_tensor_name = model_for_sim.graph.input[0].name\n",
    "finnonnx_out_tensor_name = model_for_sim.graph.output[0].name\n",
    "print(\"Input tensor name: %s\" % finnonnx_in_tensor_name)\n",
    "print(\"Output tensor name: %s\" % finnonnx_out_tensor_name)\n",
    "finnonnx_model_in_shape = model_for_sim.get_tensor_shape(finnonnx_in_tensor_name)\n",
    "finnonnx_model_out_shape = model_for_sim.get_tensor_shape(finnonnx_out_tensor_name)\n",
    "print(\"Input tensor shape: %s\" % str(finnonnx_model_in_shape))\n",
    "print(\"Output tensor shape: %s\" % str(finnonnx_model_out_shape))\n",
    "finnonnx_model_in_dt = model_for_sim.get_tensor_datatype(finnonnx_in_tensor_name)\n",
    "finnonnx_model_out_dt = model_for_sim.get_tensor_datatype(finnonnx_out_tensor_name)\n",
    "print(\"Input tensor datatype: %s\" % str(finnonnx_model_in_dt.name))\n",
    "print(\"Output tensor datatype: %s\" % str(finnonnx_model_out_dt.name))\n",
    "print(\"List of node operator types in the graph: \")\n",
    "print([x.op_type for x in model_for_sim.graph.node])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the output tensor is (as of yet) marked as a float32 value, even though we know the output is binary. This will be automatically inferred by the compiler in the next step when we run the `InferDataTypes` transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Network preparation: Tidy-up transformations <a id=\"network_preparations\"></a>\n",
    "\n",
    "prepare our FINN-ONNX model. In particular, all the intermediate tensors need to have statically defined shapes.\n",
    "\n",
    "**Graph transformations in FINN.** transform the model into a synthesizable hardware description. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from finn.transformation.general import GiveReadableTensorNames, GiveUniqueNodeNames, RemoveStaticGraphInputs\n",
    "from finn.transformation.infer_shapes import InferShapes\n",
    "from finn.transformation.infer_datatypes import InferDataTypes\n",
    "from finn.transformation.fold_constants import FoldConstants\n",
    "\n",
    "verif_model_filename = build_dir + \"/model_fmnist_notebook_verified.onnx\"\n",
    "\n",
    "model_for_sim.set_tensor_datatype(model_for_sim.graph.input[0].name, DataType[\"UINT8\"]) \n",
    "model_for_sim.save(verif_model_filename)  \n",
    "\n",
    "model_for_sim = model_for_sim.transform(InferShapes())\n",
    "model_for_sim = model_for_sim.transform(FoldConstants())\n",
    "model_for_sim = model_for_sim.transform(GiveUniqueNodeNames())\n",
    "model_for_sim = model_for_sim.transform(GiveReadableTensorNames())\n",
    "model_for_sim = model_for_sim.transform(RemoveStaticGraphInputs())\n",
    "model_for_sim.save(build_dir + \"/model_fmnist_notebook_tidy.onnx\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view our ready-to-go model after the transformations. Note that all intermediate tensors now have their shapes specified (indicated by numbers next to the arrows going between layers). Additionally, the datatype inference step has propagated quantization annotations to the outputs of `MultiThreshold` layers (expand by clicking the + next to the name of the tensor to see the quantization annotation) and the final output tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "showInNetron(build_dir + \"/model_fmnist_notebook_tidy.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "adding pre and post processing  directly in the ONNX graph. In this case, the preprocessing step divides the input uint8 data by 255 so the inputs are bounded between [0, 1]. The postprocessing step takes the output of the network and returns the index (0-9) of the image category with the highest probability (top-1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/finn-base/src/finn/transformation/infer_data_layouts.py:119: UserWarning: Assuming 4D input is NCHW\n",
      "  warnings.warn(\"Assuming 4D input is NCHW\")\n"
     ]
    }
   ],
   "source": [
    "#PREPROCESSING\n",
    "from finn.util.pytorch import ToTensor\n",
    "from finn.transformation.merge_onnx_models import MergeONNXModels\n",
    "\n",
    "model_for_sim = ModelWrapper(build_dir+\"/model_fmnist_notebook_tidy.onnx\")\n",
    "global_inp_name = model_for_sim.graph.input[0].name\n",
    "ishape = model_for_sim.get_tensor_shape(global_inp_name)\n",
    "# preprocessing: torchvision's ToTensor divides uint8 inputs by 255\n",
    "totensor_pyt = ToTensor()\n",
    "chkpt_preproc_name = build_dir + \"/model_fmnist_pre.onnx\"\n",
    "bo.export_finn_onnx(totensor_pyt, ishape, chkpt_preproc_name)\n",
    "# join preprocessing and core model\n",
    "pre_model = ModelWrapper(chkpt_preproc_name)\n",
    "model_for_sim = model_for_sim.transform(MergeONNXModels(pre_model))\n",
    "# add input quantization annotation: UINT8\n",
    "global_inp_name = model_for_sim.graph.input[0].name\n",
    "model_for_sim.set_tensor_datatype(global_inp_name, DataType[\"UINT8\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor name: global_in\n",
      "Output tensor name: global_out\n",
      "Input tensor shape: [1, 1, 28, 28]\n",
      "Output tensor shape: [1, 10]\n",
      "Input tensor datatype: UINT8\n",
      "Output tensor datatype: FLOAT32\n",
      "List of node operator types in the graph: \n",
      "['Div', 'Conv', 'Mul', 'MultiThreshold', 'Mul', 'MaxPool', 'Conv', 'Mul', 'MultiThreshold', 'Mul', 'MaxPool', 'Conv', 'Mul', 'MultiThreshold', 'Mul', 'Flatten', 'MatMul', 'Mul', 'Div', 'Add', 'Mul']\n"
     ]
    }
   ],
   "source": [
    "finnonnx_in_tensor_name = model_for_sim.graph.input[0].name\n",
    "finnonnx_out_tensor_name = model_for_sim.graph.output[0].name\n",
    "print(\"Input tensor name: %s\" % finnonnx_in_tensor_name)\n",
    "print(\"Output tensor name: %s\" % finnonnx_out_tensor_name)\n",
    "finnonnx_model_in_shape = model_for_sim.get_tensor_shape(finnonnx_in_tensor_name)\n",
    "finnonnx_model_out_shape = model_for_sim.get_tensor_shape(finnonnx_out_tensor_name)\n",
    "print(\"Input tensor shape: %s\" % str(finnonnx_model_in_shape))\n",
    "print(\"Output tensor shape: %s\" % str(finnonnx_model_out_shape))\n",
    "finnonnx_model_in_dt = model_for_sim.get_tensor_datatype(finnonnx_in_tensor_name)\n",
    "finnonnx_model_out_dt = model_for_sim.get_tensor_datatype(finnonnx_out_tensor_name)\n",
    "print(\"Input tensor datatype: %s\" % str(finnonnx_model_in_dt.name))\n",
    "print(\"Output tensor datatype: %s\" % str(finnonnx_model_out_dt.name))\n",
    "print(\"List of node operator types in the graph: \")\n",
    "print([x.op_type for x in model_for_sim.graph.node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POSTPROCESSING\n",
    "from finn.transformation.insert_topk import InsertTopK\n",
    "from finn.transformation.infer_shapes import InferShapes\n",
    "\n",
    "# postprocessing: insert Top-1 node at the end\n",
    "model_for_sim = model_for_sim.transform(InsertTopK(k=1))\n",
    "chkpt_name = build_dir+\"/model_fmnist_pre_post.onnx\"\n",
    "# tidy-up again\n",
    "model_for_sim = model_for_sim.transform(InferShapes())\n",
    "model_for_sim = model_for_sim.transform(FoldConstants())\n",
    "model_for_sim = model_for_sim.transform(GiveUniqueNodeNames())\n",
    "model_for_sim = model_for_sim.transform(GiveReadableTensorNames())\n",
    "model_for_sim = model_for_sim.transform(InferDataTypes())\n",
    "model_for_sim = model_for_sim.transform(RemoveStaticGraphInputs())\n",
    "model_for_sim.save(chkpt_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor name: global_in\n",
      "Output tensor name: global_out\n",
      "Input tensor shape: [1, 1, 28, 28]\n",
      "Output tensor shape: [1, 1]\n",
      "Input tensor datatype: UINT8\n",
      "Output tensor datatype: UINT32\n",
      "List of node operator types in the graph: \n",
      "['Div', 'Conv', 'Mul', 'MultiThreshold', 'Mul', 'MaxPool', 'Conv', 'Mul', 'MultiThreshold', 'Mul', 'MaxPool', 'Conv', 'Mul', 'MultiThreshold', 'Mul', 'Flatten', 'MatMul', 'Mul', 'Div', 'Add', 'Mul', 'TopK']\n"
     ]
    }
   ],
   "source": [
    "finnonnx_in_tensor_name = model_for_sim.graph.input[0].name\n",
    "finnonnx_out_tensor_name = model_for_sim.graph.output[0].name\n",
    "print(\"Input tensor name: %s\" % finnonnx_in_tensor_name)\n",
    "print(\"Output tensor name: %s\" % finnonnx_out_tensor_name)\n",
    "finnonnx_model_in_shape = model_for_sim.get_tensor_shape(finnonnx_in_tensor_name)\n",
    "finnonnx_model_out_shape = model_for_sim.get_tensor_shape(finnonnx_out_tensor_name)\n",
    "print(\"Input tensor shape: %s\" % str(finnonnx_model_in_shape))\n",
    "print(\"Output tensor shape: %s\" % str(finnonnx_model_out_shape))\n",
    "finnonnx_model_in_dt = model_for_sim.get_tensor_datatype(finnonnx_in_tensor_name)\n",
    "finnonnx_model_out_dt = model_for_sim.get_tensor_datatype(finnonnx_out_tensor_name)\n",
    "print(\"Input tensor datatype: %s\" % str(finnonnx_model_in_dt.name))\n",
    "print(\"Output tensor datatype: %s\" % str(finnonnx_model_out_dt.name))\n",
    "print(\"List of node operator types in the graph: \")\n",
    "print([x.op_type for x in model_for_sim.graph.node])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Serving '/workspace/finn/notebooks/mnist_ex/verification/model_fmnist_pre_post.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fcc90d97400>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(build_dir+\"/model_fmnist_pre_post.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_for_sim = ModelWrapper(build_dir + \"/model_fmnist_pre_post.onnx\")\n",
    "model_for_sim = model_for_sim.transform(MoveScalarLinearPastInvariants())\n",
    "model_for_sim = model_for_sim.transform(Streamline())\n",
    "model_for_sim = model_for_sim.transform(LowerConvsToMatMul())\n",
    "model_for_sim = model_for_sim.transform(MakeMaxPoolNHWC())\n",
    "\n",
    "model_for_sim = model_for_sim.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "\n",
    "model_for_sim = model_for_sim.transform(MakeMaxPoolNHWC())\n",
    "model_for_sim = model_for_sim.transform(absorb.AbsorbTransposeIntoMultiThreshold())\n",
    "\n",
    "model_for_sim = model_for_sim.transform(Streamline())\n",
    "# absorb final add-mul nodes into TopK\n",
    "model_for_sim = model_for_sim.transform(absorb.AbsorbScalarMulAddIntoTopK())\n",
    "model_for_sim = model_for_sim.transform(InferDataLayouts())\n",
    "model_for_sim = model_for_sim.transform(RemoveUnusedTensors())\n",
    "\n",
    "model_for_sim.save(build_dir + \"/model_fmnist_stream.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/workspace/finn/notebooks/mnist_ex/verification/model_fmnist_stream.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fcc7372ef10>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "showInNetron(build_dir+\"/model_fmnist_stream.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HLS\n",
    "model_for_sim = ModelWrapper(build_dir + \"/model_fmnist_stream.onnx\")\n",
    "\n",
    "# choose the memory mode for the MVTU units, decoupled or const\n",
    "mem_mode = \"decoupled\"\n",
    "\n",
    "\n",
    "model_for_sim = model_for_sim.transform(to_hls.InferQuantizedStreamingFCLayer(mem_mode))\n",
    "# TopK to LabelSelect\n",
    "model_for_sim = model_for_sim.transform(to_hls.InferLabelSelectLayer())\n",
    "# input quantization (if any) to standalone thresholding\n",
    "\n",
    "\n",
    "model_for_sim = model_for_sim.transform(to_hls.InferStreamingMaxPool())\n",
    "model_for_sim = model_for_sim.transform(to_hls.InferPool_Batch())\n",
    "\n",
    "model_for_sim = model_for_sim.transform(to_hls.InferThresholdingLayer())\n",
    "model_for_sim = model_for_sim.transform(to_hls.InferConvInpGen())\n",
    "model_for_sim = model_for_sim.transform(to_hls.InferStreamingMaxPool())\n",
    "# get rid of Reshape(-1, 1) operation between hlslib nodes\n",
    "model_for_sim = model_for_sim.transform(RemoveCNVtoFCFlatten())\n",
    "# get rid of Tranpose -> Tranpose identity seq\n",
    "model_for_sim = model_for_sim.transform(absorb.AbsorbConsecutiveTransposes())\n",
    "# infer tensor data layouts\n",
    "model_for_sim = model_for_sim.transform(InferDataLayouts())\n",
    "\n",
    "\n",
    "model_for_sim = model_for_sim.transform(InferDataTypes())\n",
    "model_for_sim = model_for_sim.transform(RemoveStaticGraphInputs())\n",
    "model_for_sim = model_for_sim.transform(RemoveUnusedTensors())\n",
    "model_for_sim.save(build_dir + \"/model_fmnist_final.onnx\")\n",
    "parent_model = model_for_sim.transform(CreateDataflowPartition())\n",
    "parent_model.save(build_dir + \"/dataflow_parent.onnx\")\n",
    "sdp_node = parent_model.get_nodes_by_op_type(\"StreamingDataflowPartition\")[0]\n",
    "sdp_node = getCustomOp(sdp_node)\n",
    "dataflow_model_filename = sdp_node.get_nodeattr(\"model\")\n",
    "# save the dataflow partition with a different name for easier access\n",
    "dataflow_model = ModelWrapper(dataflow_model_filename)\n",
    "dataflow_model.save(build_dir + \"/dataflow_model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping http://0.0.0.0:8081\n",
      "Serving '/workspace/finn/notebooks/mnist_ex/verification/model_fmnist_final.onnx' at http://0.0.0.0:8081\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"400\"\n",
       "            src=\"http://localhost:8081/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fcd9c6d3610>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from finn.util.visualization import showInNetron\n",
    "\n",
    "showInNetron(build_dir + \"/model_fmnist_final.onnx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Load the Dataset and the Brevitas Model <a id=\"load_dataset\"></a>\n",
    "\n",
    "We'll use some example data from the quantized UNSW-NB15 dataset (from the previous notebook) to use as inputs for the verification. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import model .onnx into FINN\n",
    "from finn.core.modelwrapper import ModelWrapper\n",
    "build_dir=\"./verification\"\n",
    "model_for_sim = ModelWrapper(build_dir + \"/model_fmnist_notebook_tidy.onnx\")\n",
    "from dataset_loading import mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312, 32, 784)\n"
     ]
    }
   ],
   "source": [
    "bsize=32\n",
    "dataset_root=\"/workspace/finn/notebooks/mnist_ex/data2/FashionMNIST/raw\"\n",
    "\n",
    "trainx, trainy, testx, testy, valx, valy = mnist.load_mnist_data(dataset_root, download=False, one_hot=False)\n",
    "\n",
    "test_imgs = testx\n",
    "test_labels = testy\n",
    "total = test_imgs.shape[0]\n",
    "n_batches = int(total / bsize)\n",
    "limit = n_batches*bsize\n",
    "test_imgs = test_imgs[:limit,:,:].reshape(n_batches, bsize, -1)\n",
    "test_labels = test_labels[:limit,].reshape(n_batches, bsize)\n",
    "print(test_imgs.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input tensor name: global_in\n",
      "Output tensor name: global_out\n",
      "Input tensor shape: [1, 1, 28, 28]\n",
      "Output tensor shape: [1, 10]\n",
      "Input tensor datatype: UINT8\n",
      "Output tensor datatype: FLOAT32\n",
      "List of node operator types in the graph: \n",
      "['Conv', 'Mul', 'MultiThreshold', 'Mul', 'MaxPool', 'Conv', 'Mul', 'MultiThreshold', 'Mul', 'MaxPool', 'Conv', 'Mul', 'MultiThreshold', 'Mul', 'Flatten', 'MatMul', 'Mul', 'Div', 'Add', 'Mul']\n"
     ]
    }
   ],
   "source": [
    "finnonnx_in_tensor_name = model_for_sim.graph.input[0].name\n",
    "finnonnx_out_tensor_name = model_for_sim.graph.output[0].name\n",
    "print(\"Input tensor name: %s\" % finnonnx_in_tensor_name)\n",
    "print(\"Output tensor name: %s\" % finnonnx_out_tensor_name)\n",
    "finnonnx_model_in_shape = model_for_sim.get_tensor_shape(finnonnx_in_tensor_name)\n",
    "finnonnx_model_out_shape = model_for_sim.get_tensor_shape(finnonnx_out_tensor_name)\n",
    "print(\"Input tensor shape: %s\" % str(finnonnx_model_in_shape))\n",
    "print(\"Output tensor shape: %s\" % str(finnonnx_model_out_shape))\n",
    "finnonnx_model_in_dt = model_for_sim.get_tensor_datatype(finnonnx_in_tensor_name)\n",
    "finnonnx_model_out_dt = model_for_sim.get_tensor_datatype(finnonnx_out_tensor_name)\n",
    "print(\"Input tensor datatype: %s\" % str(finnonnx_model_in_dt.name))\n",
    "print(\"Output tensor datatype: %s\" % str(finnonnx_model_out_dt.name))\n",
    "print(\"List of node operator types in the graph: \")\n",
    "print([x.op_type for x in model_for_sim.graph.node])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also bring up the MLP we trained in Brevitas from the previous notebook. We'll compare its outputs to what is generated by FINN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#MODEL\n",
    "\n",
    "from torch import nn\n",
    "from torch.nn import Module\n",
    "import brevitas.nn as qnn\n",
    "from brevitas.quant import Int8Bias as BiasQuant\n",
    "from brevitas.quant import Uint8ActPerTensorFloat as ActQuant\n",
    "from brevitas.quant import Int8WeightPerTensorFloat as WeighQuant\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Setting seeds for reproducibility\n",
    "torch.manual_seed(0)\n",
    "\n",
    "weight_bitx = 8\n",
    "act_bitx = 8\n",
    "bias_bitx = 8\n",
    "\n",
    "model = nn.Sequential(\n",
    "    #qnn.QuantIdentity(bit_width=act_bitx, return_quant_tensor=True),\n",
    "    qnn.QuantConv2d(in_channels=1, out_channels=3, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2),\n",
    "                                     bias=False, weight_bit_width=weight_bitx, weight_quant=WeighQuant,\n",
    "                                     bias_quant=BiasQuant, return_quant_tensor=True),\n",
    "    qnn.QuantReLU(bit_width=act_bitx, act_quant=ActQuant, return_quant_tensor=True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    qnn.QuantConv2d(in_channels=3, out_channels=8, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2),\n",
    "                                     bias=False, weight_bit_width=weight_bitx, bias_quant=BiasQuant,\n",
    "                                     weight_quant=WeighQuant, return_quant_tensor=True),\n",
    "    qnn.QuantReLU(bit_width=act_bitx, act_quant=ActQuant, return_quant_tensor=True),\n",
    "    nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "    qnn.QuantConv2d(in_channels=8, out_channels=16, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2),\n",
    "                                     bias=False, weight_bit_width=weight_bitx, bias_quant=BiasQuant,\n",
    "                                     weight_quant=WeighQuant, return_quant_tensor=True),\n",
    "    qnn.QuantReLU(bit_width=act_bitx, act_quant=ActQuant, return_quant_tensor=True),\n",
    "    nn.Flatten(),\n",
    "    qnn.QuantLinear(in_features=16*7*7, out_features=10, bias=True, weight_bit_width=weight_bitx,\n",
    "                                   weight_quant=WeighQuant, bias_quant=BiasQuant, return_quant_tensor=False)\n",
    ")\n",
    "trained_state_dict = torch.load(\"state_dict_self-trained.pth\")\n",
    "#trained_state_dict = torch.load(\"state_dict.pth\")[\"models_state_dict\"][0]\n",
    "model.load_state_dict(trained_state_dict, strict=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_with_brevitas(current_inp):\n",
    "    input_tensors = current_inp.astype(np.float32)\n",
    "    input_tensors = torch.from_numpy(input_tensors)\n",
    "    brevitas_output = model.forward(input_tensors)\n",
    "    y_pred=brevitas_output.argmax(1)\n",
    "    return y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Compare FINN & Brevitas execution <a id=\"compare_brevitas\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make helper functions to execute the same input with Brevitas and FINN. For FINN, we'll use the [`finn.core.onnx_exec`](https://finn.readthedocs.io/en/latest/source_code/finn.core.html#finn.core.onnx_exec.execute_onnx) function to execute the exported FINN-ONNX on the inputs. Note that this ONNX execution is for verification only; not for accelerated execution.\n",
    "\n",
    "Recall that the quantized values from the dataset are 593-bit binary {0, 1} vectors whereas our exported model takes 600-bit bipolar {-1, +1} vectors, so we'll have to preprocess it a bit before we can use it for verifying the ONNX model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import finn.core.onnx_exec as oxe\n",
    "\n",
    "def inference_with_finn_onnx(current_inp):\n",
    "    finnonnx_in_tensor_name = model_for_sim.graph.input[0].name\n",
    "    finnonnx_model_in_shape = model_for_sim.get_tensor_shape(finnonnx_in_tensor_name)\n",
    "    finnonnx_out_tensor_name = model_for_sim.graph.output[0].name\n",
    "    #print(finnonnx_model_in_shape)\n",
    "    current_inp=current_inp.astype(np.float32)\n",
    "    # reshape to expected input (add 1 for batch dimension)\n",
    "    current_inp = current_inp.reshape(finnonnx_model_in_shape)\n",
    "    # create the input dictionary\n",
    "    input_dict = {finnonnx_in_tensor_name : current_inp} \n",
    "    # run with FINN's execute_onnx\n",
    "    output_dict = oxe.execute_onnx(model_for_sim, input_dict)\n",
    "    #get the output tensor\n",
    "    finn_output = output_dict[finnonnx_out_tensor_name]  \n",
    "    finn_output= torch.from_numpy(finn_output.argmax(1))\n",
    "    return finn_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can call our inference helper functions for each input and compare the outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ok 32 nok 0: 100%|██████████| 32/32 [00:03<00:00, 10.50it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from tqdm import trange\n",
    "\n",
    "n_verification_inputs=32\n",
    "verify_range = trange(n_verification_inputs, desc=\"FINN execution\", position=0, leave=True)\n",
    "model.eval()\n",
    "\n",
    "ok = 0\n",
    "nok = 0\n",
    "\n",
    "for i in verify_range:\n",
    "    current_inp = test_imgs[0][i].reshape(1, 1, 28, 28)\n",
    "    brevitas_output = inference_with_brevitas(current_inp)\n",
    "    finn_output = inference_with_finn_onnx(current_inp)\n",
    "    #print(brevitas_output, finn_output)\n",
    "    # compare the outputs\n",
    "    ok += 1 if finn_output == brevitas_output else 0\n",
    "    nok += 1 if finn_output != brevitas_output else 0\n",
    "    verify_range.set_description(\"ok %d nok %d\" % (ok, nok))\n",
    "    verify_range.refresh()\n",
    "    #print(finn_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification succeeded. Brevitas and FINN-ONNX execution outputs are identical\n"
     ]
    }
   ],
   "source": [
    "if ok == n_verification_inputs:\n",
    "    print(\"Verification succeeded. Brevitas and FINN-ONNX execution outputs are identical\")\n",
    "else:\n",
    "    print(\"Verification failed. Brevitas and FINN-ONNX execution outputs are NOT identical\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This concludes our second notebook. In the next one, we'll take the ONNX model we just verified all the way down to FPGA hardware with the FINN compiler."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
